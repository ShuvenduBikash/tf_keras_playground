{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec: SkipGram Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NISzsIFOIW__",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LboYzLtSh-D",
        "colab_type": "code",
        "outputId": "97e827d0-8dc0-484e-9d67-574b8f968a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpFBxs0TXuyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4C_WFRQDGS5",
        "colab_type": "code",
        "outputId": "f647cfe5-cd76-443b-bdc9-fb8764aad3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "\n",
        "bible = gutenberg.sents('bible-kjv.txt') \n",
        "remove_terms = punctuation + '0123456789'\n",
        "\n",
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
        "\n",
        "print('Total lines:', len(bible))\n",
        "print('\\nSample line:', bible[10])\n",
        "print('\\nProcessed line:', norm_bible[10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total lines: 30103\n",
            "\n",
            "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
            "\n",
            "Processed line: god said let firmament midst waters let divide waters waters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1567Lg1jOnH8",
        "colab_type": "code",
        "outputId": "43caa3b7-cb80-4f88-9bd2-0dbd2029d490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "# build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULtFu7m_TMna",
        "colab_type": "text"
      },
      "source": [
        "# Build a skip-gram [(target, context), relevancy] generator\n",
        "\n",
        "For this, we feed our skip-gram model pairs of (X, Y) where X is our input and Y is our label. We do this by using [(target, context), 1] pairs as positive input samples where target is our word of interest and context is a context word occurring near the target word and the positive label 1 indicates this is a contextually relevant pair. We also feed in [(target, random), 0] pairs as negative input samples where target is again our word of interest but random is just a randomly selected word from our vocabulary which has no context or association with our target word. Hence the negative label 0 indicates this is a contextually irrelevant pair. We do this so that the model can then learn which pairs of words are contextually relevant and which are not and generate similar embeddings for semantically similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtLA4k9eR6tM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3f41b0fb-730f-4a77-f34c-6d77f6d93ab3"
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "# generate skip-grams\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(bible (5766), james (1154)) -> 1\n",
            "(james (1154), rohgah (9806)) -> 0\n",
            "(bible (5766), korahite (9872)) -> 0\n",
            "(king (13), eber (2798)) -> 0\n",
            "(james (1154), bible (5766)) -> 1\n",
            "(king (13), bible (5766)) -> 1\n",
            "(king (13), james (1154)) -> 1\n",
            "(bible (5766), king (13)) -> 1\n",
            "(james (1154), amends (8715)) -> 0\n",
            "(james (1154), king (13)) -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaqExYSn3P1M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cec9e0a8-9e08-4a06-a800-bf39b699260a"
      },
      "source": [
        "len(skip_grams)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA17acve3R3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "699e6f75-8caf-41f9-c731-5ae026942511"
      },
      "source": [
        "skip_grams[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[5766, 1154],\n",
              "  [1154, 9806],\n",
              "  [5766, 9872],\n",
              "  [13, 2798],\n",
              "  [1154, 5766],\n",
              "  [13, 5766],\n",
              "  [13, 1154],\n",
              "  [5766, 13],\n",
              "  [1154, 8715],\n",
              "  [1154, 13],\n",
              "  [13, 4529],\n",
              "  [5766, 8840]],\n",
              " [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urrMuWWJLkns",
        "colab_type": "text"
      },
      "source": [
        "# Building the skip-gram model architecture\n",
        "\n",
        " For this our inputs will be our target word and context or random word pair. Each of which are passed to an embedding layer (initialized with random weights) of it’s own. Once we obtain the word embeddings for the target and the context word, we pass it to a merge layer where we compute the dot product of these two vectors. Then we pass on this dot product value to a dense sigmoid layer which predicts either a 1 or a 0 depending on if the pair of words are contextually relevant or just random words (Y’). We match this with the actual relevance label (Y), compute the loss by leveraging the mean_squared_error loss and perform backpropagation with each epoch to update the embedding layer in the process. Following code shows us our model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K4XIqA7MQ2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "ebac8a97-7f86-42d6-dec3-db33de9ef9d9"
      },
      "source": [
        "from keras.layers import dot\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, Input\n",
        "from keras.models import Model\n",
        "\n",
        "input_1 = Input(shape=(1,))\n",
        "input_2 = Input(shape=(1,))\n",
        "\n",
        "# build skip-gram architecture\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "word_model.summary()\n",
        "\n",
        "out1 = word_model(input_1)\n",
        "\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "out2 = context_model(input_2)\n",
        "\n",
        "x = dot([out1, out2], axes=1)\n",
        "x = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(x)\n",
        "\n",
        "model =  Model(inputs=[input_1, input_2], outputs=x)\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
        "                  rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_47 (Embedding)     (None, 1, 100)            1242500   \n",
            "_________________________________________________________________\n",
            "reshape_47 (Reshape)         (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 1,242,500\n",
            "Trainable params: 1,242,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_63 (Sequential)      (None, 100)          1242500     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_64 (Sequential)      (None, 100)          1242500     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_16 (Dot)                    (None, 1)            0           sequential_63[1][0]              \n",
            "                                                                 sequential_64[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           dot_16[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,485,002\n",
            "Trainable params: 2,485,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"304pt\" viewBox=\"0.00 0.00 470.00 304.00\" width=\"470pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 466,-300 466,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139829279639032 -->\n<g class=\"node\" id=\"node1\">\n<title>139829279639032</title>\n<polygon fill=\"none\" points=\"6,-249.5 6,-295.5 216,-295.5 216,-249.5 6,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"86,-249.5 86,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"86,-272.5 144,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"144,-249.5 144,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"144,-272.5 216,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180\" y=\"-257.3\">(None, 1)</text>\n</g>\n<!-- 139829279300072 -->\n<g class=\"node\" id=\"node3\">\n<title>139829279300072</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 222,-212.5 222,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"38.5\" y=\"-185.8\">Sequential</text>\n<polyline fill=\"none\" points=\"77,-166.5 77,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"77,-189.5 135,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"135,-166.5 135,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-197.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"135,-189.5 222,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139829279639032&#45;&gt;139829279300072 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139829279639032-&gt;139829279300072</title>\n<path d=\"M111,-249.3799C111,-241.1745 111,-231.7679 111,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"114.5001,-222.784 111,-212.784 107.5001,-222.784 114.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139829281723112 -->\n<g class=\"node\" id=\"node2\">\n<title>139829281723112</title>\n<polygon fill=\"none\" points=\"246,-249.5 246,-295.5 456,-295.5 456,-249.5 246,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"326,-249.5 326,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"326,-272.5 384,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"384,-249.5 384,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"384,-272.5 456,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-257.3\">(None, 1)</text>\n</g>\n<!-- 139829279300800 -->\n<g class=\"node\" id=\"node4\">\n<title>139829279300800</title>\n<polygon fill=\"none\" points=\"240,-166.5 240,-212.5 462,-212.5 462,-166.5 240,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-185.8\">Sequential</text>\n<polyline fill=\"none\" points=\"317,-166.5 317,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"317,-189.5 375,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"375,-166.5 375,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-197.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"375,-189.5 462,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139829281723112&#45;&gt;139829279300800 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139829281723112-&gt;139829279300800</title>\n<path d=\"M351,-249.3799C351,-241.1745 351,-231.7679 351,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"354.5001,-222.784 351,-212.784 347.5001,-222.784 354.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279299848 -->\n<g class=\"node\" id=\"node5\">\n<title>139829279299848</title>\n<polygon fill=\"none\" points=\"96,-83.5 96,-129.5 366,-129.5 366,-83.5 96,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-102.8\">Dot</text>\n<polyline fill=\"none\" points=\"134,-83.5 134,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-106.5 192,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279\" y=\"-114.3\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" points=\"192,-106.5 366,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279\" y=\"-91.3\">(None, 1)</text>\n</g>\n<!-- 139829279300072&#45;&gt;139829279299848 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139829279300072-&gt;139829279299848</title>\n<path d=\"M144.4267,-166.3799C158.224,-156.8367 174.3707,-145.6686 188.9576,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"191.103,-138.3511 197.3364,-129.784 187.121,-132.594 191.103,-138.3511\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279300800&#45;&gt;139829279299848 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139829279300800-&gt;139829279299848</title>\n<path d=\"M317.5733,-166.3799C303.776,-156.8367 287.6293,-145.6686 273.0424,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"274.879,-132.594 264.6636,-129.784 270.897,-138.3511 274.879,-132.594\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279298952 -->\n<g class=\"node\" id=\"node6\">\n<title>139829279298952</title>\n<polygon fill=\"none\" points=\"140,-.5 140,-46.5 322,-46.5 322,-.5 140,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"192,-.5 192,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"192,-23.5 250,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"250,-.5 250,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-31.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"250,-23.5 322,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 139829279299848&#45;&gt;139829279298952 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139829279299848-&gt;139829279298952</title>\n<path d=\"M231,-83.3799C231,-75.1745 231,-65.7679 231,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"234.5001,-56.784 231,-46.784 227.5001,-56.784 234.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHusQnbfd0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "3d17a99e-03ff-43ed-88e7-3735464878ec"
      },
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0710 06:13:51.172223 139832561526656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0710 06:13:51.204910 139832561526656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 4531.9908620379865\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 3767.3675716863945\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 3753.8449253009167\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 3804.7816245462163\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
            "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 3742.432873359561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgG9UgwTkXG-",
        "colab_type": "text"
      },
      "source": [
        "# Get Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-DjPibykZLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "e65bde08-2346-4feb-c0cf-fee1542b85f5"
      },
      "source": [
        "word_embed_layer = word_model.layers[0]\n",
        "weights = word_embed_layer.get_weights()[0]\n",
        "\n",
        "print(weights.shape)\n",
        "pd.DataFrame(weights, index=id2word.values()).head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12425, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>shall</th>\n",
              "      <td>-0.012422</td>\n",
              "      <td>-0.020036</td>\n",
              "      <td>-0.017483</td>\n",
              "      <td>-0.013122</td>\n",
              "      <td>0.010576</td>\n",
              "      <td>-0.007503</td>\n",
              "      <td>0.011215</td>\n",
              "      <td>0.007315</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>-0.017951</td>\n",
              "      <td>-0.016464</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.006184</td>\n",
              "      <td>-0.020151</td>\n",
              "      <td>-0.005745</td>\n",
              "      <td>-0.010562</td>\n",
              "      <td>-0.017033</td>\n",
              "      <td>-0.003692</td>\n",
              "      <td>-0.016533</td>\n",
              "      <td>-0.005889</td>\n",
              "      <td>-0.008186</td>\n",
              "      <td>0.014368</td>\n",
              "      <td>0.008513</td>\n",
              "      <td>-0.008814</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>-0.018742</td>\n",
              "      <td>0.009084</td>\n",
              "      <td>0.004849</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>0.009122</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>-0.013275</td>\n",
              "      <td>-0.000635</td>\n",
              "      <td>-0.005764</td>\n",
              "      <td>-0.006347</td>\n",
              "      <td>0.012258</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.019806</td>\n",
              "      <td>0.010275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017121</td>\n",
              "      <td>-0.008911</td>\n",
              "      <td>0.013081</td>\n",
              "      <td>-0.000067</td>\n",
              "      <td>0.016754</td>\n",
              "      <td>-0.014832</td>\n",
              "      <td>-0.007994</td>\n",
              "      <td>-0.017931</td>\n",
              "      <td>-0.010790</td>\n",
              "      <td>0.004147</td>\n",
              "      <td>-0.008141</td>\n",
              "      <td>-0.011672</td>\n",
              "      <td>0.012713</td>\n",
              "      <td>-0.018913</td>\n",
              "      <td>0.018869</td>\n",
              "      <td>0.010652</td>\n",
              "      <td>-0.018133</td>\n",
              "      <td>0.020519</td>\n",
              "      <td>-0.015392</td>\n",
              "      <td>-0.006466</td>\n",
              "      <td>0.003543</td>\n",
              "      <td>-0.010969</td>\n",
              "      <td>-0.014942</td>\n",
              "      <td>-0.020452</td>\n",
              "      <td>-0.011605</td>\n",
              "      <td>0.020499</td>\n",
              "      <td>0.009112</td>\n",
              "      <td>0.003567</td>\n",
              "      <td>0.019239</td>\n",
              "      <td>0.011071</td>\n",
              "      <td>-0.008868</td>\n",
              "      <td>0.007840</td>\n",
              "      <td>0.006471</td>\n",
              "      <td>-0.010983</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>-0.011551</td>\n",
              "      <td>-0.008325</td>\n",
              "      <td>-0.017947</td>\n",
              "      <td>-0.004696</td>\n",
              "      <td>0.018255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unto</th>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.180420</td>\n",
              "      <td>-0.441217</td>\n",
              "      <td>0.037462</td>\n",
              "      <td>0.127804</td>\n",
              "      <td>0.032331</td>\n",
              "      <td>0.003542</td>\n",
              "      <td>-0.043264</td>\n",
              "      <td>0.039325</td>\n",
              "      <td>0.041041</td>\n",
              "      <td>-0.110801</td>\n",
              "      <td>-0.199340</td>\n",
              "      <td>-0.045121</td>\n",
              "      <td>-0.110810</td>\n",
              "      <td>-0.057338</td>\n",
              "      <td>0.023514</td>\n",
              "      <td>-0.044591</td>\n",
              "      <td>-0.002689</td>\n",
              "      <td>0.013096</td>\n",
              "      <td>-0.100149</td>\n",
              "      <td>0.155493</td>\n",
              "      <td>-0.002136</td>\n",
              "      <td>-0.168873</td>\n",
              "      <td>-0.059107</td>\n",
              "      <td>-0.099961</td>\n",
              "      <td>-0.032566</td>\n",
              "      <td>-0.174667</td>\n",
              "      <td>-0.046394</td>\n",
              "      <td>-0.039652</td>\n",
              "      <td>0.033117</td>\n",
              "      <td>-0.005316</td>\n",
              "      <td>-0.027812</td>\n",
              "      <td>-0.158080</td>\n",
              "      <td>-0.005536</td>\n",
              "      <td>-0.143310</td>\n",
              "      <td>-0.158454</td>\n",
              "      <td>-0.065740</td>\n",
              "      <td>0.038800</td>\n",
              "      <td>0.039770</td>\n",
              "      <td>-0.016265</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009764</td>\n",
              "      <td>-0.124887</td>\n",
              "      <td>-0.099747</td>\n",
              "      <td>-0.014948</td>\n",
              "      <td>-0.092568</td>\n",
              "      <td>0.160481</td>\n",
              "      <td>-0.191145</td>\n",
              "      <td>-0.069600</td>\n",
              "      <td>-0.225256</td>\n",
              "      <td>0.044475</td>\n",
              "      <td>-0.045760</td>\n",
              "      <td>0.012984</td>\n",
              "      <td>0.089087</td>\n",
              "      <td>0.123042</td>\n",
              "      <td>-0.064627</td>\n",
              "      <td>0.190248</td>\n",
              "      <td>-0.130555</td>\n",
              "      <td>-0.344315</td>\n",
              "      <td>0.007837</td>\n",
              "      <td>0.027703</td>\n",
              "      <td>0.152754</td>\n",
              "      <td>-0.004223</td>\n",
              "      <td>0.154084</td>\n",
              "      <td>-0.123652</td>\n",
              "      <td>0.039304</td>\n",
              "      <td>-0.010693</td>\n",
              "      <td>0.278337</td>\n",
              "      <td>-0.058895</td>\n",
              "      <td>-0.089672</td>\n",
              "      <td>0.030795</td>\n",
              "      <td>0.103642</td>\n",
              "      <td>0.192906</td>\n",
              "      <td>-0.010108</td>\n",
              "      <td>-0.112736</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>0.087188</td>\n",
              "      <td>0.062247</td>\n",
              "      <td>0.254126</td>\n",
              "      <td>-0.141179</td>\n",
              "      <td>-0.051032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lord</th>\n",
              "      <td>0.061889</td>\n",
              "      <td>0.077935</td>\n",
              "      <td>-0.281119</td>\n",
              "      <td>0.008736</td>\n",
              "      <td>0.044355</td>\n",
              "      <td>0.013745</td>\n",
              "      <td>0.136037</td>\n",
              "      <td>-0.018273</td>\n",
              "      <td>0.067286</td>\n",
              "      <td>0.224294</td>\n",
              "      <td>-0.003325</td>\n",
              "      <td>-0.098153</td>\n",
              "      <td>-0.028425</td>\n",
              "      <td>-0.161629</td>\n",
              "      <td>-0.064775</td>\n",
              "      <td>0.163440</td>\n",
              "      <td>-0.097830</td>\n",
              "      <td>-0.040454</td>\n",
              "      <td>-0.032482</td>\n",
              "      <td>0.013378</td>\n",
              "      <td>-0.001417</td>\n",
              "      <td>0.001955</td>\n",
              "      <td>-0.011842</td>\n",
              "      <td>-0.054225</td>\n",
              "      <td>-0.062158</td>\n",
              "      <td>-0.027082</td>\n",
              "      <td>-0.411419</td>\n",
              "      <td>-0.018333</td>\n",
              "      <td>-0.014946</td>\n",
              "      <td>0.048303</td>\n",
              "      <td>-0.014260</td>\n",
              "      <td>0.033983</td>\n",
              "      <td>-0.049177</td>\n",
              "      <td>0.112251</td>\n",
              "      <td>0.115364</td>\n",
              "      <td>0.008551</td>\n",
              "      <td>-0.045735</td>\n",
              "      <td>0.167213</td>\n",
              "      <td>0.096660</td>\n",
              "      <td>0.029182</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031148</td>\n",
              "      <td>0.023146</td>\n",
              "      <td>-0.048519</td>\n",
              "      <td>0.021851</td>\n",
              "      <td>-0.131256</td>\n",
              "      <td>0.002964</td>\n",
              "      <td>-0.011669</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>-0.187086</td>\n",
              "      <td>-0.034647</td>\n",
              "      <td>-0.220000</td>\n",
              "      <td>0.015831</td>\n",
              "      <td>0.085446</td>\n",
              "      <td>0.026685</td>\n",
              "      <td>-0.022796</td>\n",
              "      <td>-0.057739</td>\n",
              "      <td>0.122991</td>\n",
              "      <td>-0.283213</td>\n",
              "      <td>0.080222</td>\n",
              "      <td>-0.095072</td>\n",
              "      <td>0.097093</td>\n",
              "      <td>-0.009105</td>\n",
              "      <td>0.027330</td>\n",
              "      <td>0.033144</td>\n",
              "      <td>-0.022108</td>\n",
              "      <td>0.007612</td>\n",
              "      <td>0.198059</td>\n",
              "      <td>0.069299</td>\n",
              "      <td>-0.249739</td>\n",
              "      <td>-0.055430</td>\n",
              "      <td>-0.129567</td>\n",
              "      <td>0.097200</td>\n",
              "      <td>0.083280</td>\n",
              "      <td>-0.052149</td>\n",
              "      <td>-0.106127</td>\n",
              "      <td>-0.078064</td>\n",
              "      <td>-0.113166</td>\n",
              "      <td>0.215920</td>\n",
              "      <td>-0.139021</td>\n",
              "      <td>-0.034943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thou</th>\n",
              "      <td>0.090094</td>\n",
              "      <td>0.344583</td>\n",
              "      <td>-0.355620</td>\n",
              "      <td>0.106176</td>\n",
              "      <td>-0.029886</td>\n",
              "      <td>-0.012042</td>\n",
              "      <td>-0.016372</td>\n",
              "      <td>0.032659</td>\n",
              "      <td>0.195226</td>\n",
              "      <td>0.013223</td>\n",
              "      <td>0.026530</td>\n",
              "      <td>-0.074357</td>\n",
              "      <td>-0.119079</td>\n",
              "      <td>-0.072989</td>\n",
              "      <td>-0.058612</td>\n",
              "      <td>0.117646</td>\n",
              "      <td>-0.111835</td>\n",
              "      <td>-0.033928</td>\n",
              "      <td>-0.009969</td>\n",
              "      <td>-0.159031</td>\n",
              "      <td>-0.060775</td>\n",
              "      <td>-0.020605</td>\n",
              "      <td>-0.125407</td>\n",
              "      <td>-0.177898</td>\n",
              "      <td>0.037218</td>\n",
              "      <td>-0.081958</td>\n",
              "      <td>-0.220567</td>\n",
              "      <td>0.004148</td>\n",
              "      <td>-0.064559</td>\n",
              "      <td>-0.014126</td>\n",
              "      <td>-0.013032</td>\n",
              "      <td>0.151565</td>\n",
              "      <td>-0.232864</td>\n",
              "      <td>0.082738</td>\n",
              "      <td>0.053368</td>\n",
              "      <td>-0.141257</td>\n",
              "      <td>-0.159900</td>\n",
              "      <td>0.202295</td>\n",
              "      <td>0.072843</td>\n",
              "      <td>0.088620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049397</td>\n",
              "      <td>-0.014007</td>\n",
              "      <td>0.045505</td>\n",
              "      <td>0.096238</td>\n",
              "      <td>-0.059895</td>\n",
              "      <td>0.058028</td>\n",
              "      <td>-0.049784</td>\n",
              "      <td>-0.083291</td>\n",
              "      <td>-0.023573</td>\n",
              "      <td>0.036988</td>\n",
              "      <td>-0.024366</td>\n",
              "      <td>-0.035928</td>\n",
              "      <td>0.034990</td>\n",
              "      <td>0.097629</td>\n",
              "      <td>-0.153647</td>\n",
              "      <td>0.016012</td>\n",
              "      <td>-0.023025</td>\n",
              "      <td>-0.134979</td>\n",
              "      <td>0.030312</td>\n",
              "      <td>-0.152775</td>\n",
              "      <td>0.123341</td>\n",
              "      <td>-0.002578</td>\n",
              "      <td>0.089710</td>\n",
              "      <td>-0.079110</td>\n",
              "      <td>0.015033</td>\n",
              "      <td>0.017928</td>\n",
              "      <td>0.075198</td>\n",
              "      <td>-0.040014</td>\n",
              "      <td>-0.094308</td>\n",
              "      <td>0.114139</td>\n",
              "      <td>-0.018035</td>\n",
              "      <td>0.090245</td>\n",
              "      <td>0.075862</td>\n",
              "      <td>-0.001541</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>-0.153127</td>\n",
              "      <td>0.026079</td>\n",
              "      <td>0.185531</td>\n",
              "      <td>-0.040391</td>\n",
              "      <td>0.029416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thy</th>\n",
              "      <td>0.078411</td>\n",
              "      <td>0.065561</td>\n",
              "      <td>-0.414808</td>\n",
              "      <td>0.023220</td>\n",
              "      <td>-0.113078</td>\n",
              "      <td>0.038957</td>\n",
              "      <td>0.091203</td>\n",
              "      <td>-0.227318</td>\n",
              "      <td>0.054084</td>\n",
              "      <td>0.022882</td>\n",
              "      <td>-0.029323</td>\n",
              "      <td>-0.076458</td>\n",
              "      <td>0.036480</td>\n",
              "      <td>-0.019148</td>\n",
              "      <td>-0.254878</td>\n",
              "      <td>0.058988</td>\n",
              "      <td>-0.073930</td>\n",
              "      <td>0.011486</td>\n",
              "      <td>-0.125697</td>\n",
              "      <td>-0.042732</td>\n",
              "      <td>-0.054411</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>-0.289262</td>\n",
              "      <td>-0.009530</td>\n",
              "      <td>0.080376</td>\n",
              "      <td>-0.047801</td>\n",
              "      <td>-0.300094</td>\n",
              "      <td>0.076672</td>\n",
              "      <td>-0.034164</td>\n",
              "      <td>0.113948</td>\n",
              "      <td>-0.026071</td>\n",
              "      <td>0.051913</td>\n",
              "      <td>-0.045160</td>\n",
              "      <td>0.035580</td>\n",
              "      <td>-0.020676</td>\n",
              "      <td>-0.140302</td>\n",
              "      <td>-0.098480</td>\n",
              "      <td>0.160988</td>\n",
              "      <td>-0.029239</td>\n",
              "      <td>0.012719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.041800</td>\n",
              "      <td>-0.023576</td>\n",
              "      <td>0.012160</td>\n",
              "      <td>0.018446</td>\n",
              "      <td>-0.267084</td>\n",
              "      <td>0.100041</td>\n",
              "      <td>-0.003852</td>\n",
              "      <td>-0.042449</td>\n",
              "      <td>-0.156670</td>\n",
              "      <td>-0.141122</td>\n",
              "      <td>-0.034390</td>\n",
              "      <td>-0.110718</td>\n",
              "      <td>0.045714</td>\n",
              "      <td>0.132798</td>\n",
              "      <td>0.096027</td>\n",
              "      <td>0.151834</td>\n",
              "      <td>0.057732</td>\n",
              "      <td>-0.192624</td>\n",
              "      <td>0.102324</td>\n",
              "      <td>0.064853</td>\n",
              "      <td>0.351041</td>\n",
              "      <td>-0.014989</td>\n",
              "      <td>0.006020</td>\n",
              "      <td>-0.014475</td>\n",
              "      <td>0.016563</td>\n",
              "      <td>0.073243</td>\n",
              "      <td>-0.087481</td>\n",
              "      <td>-0.057189</td>\n",
              "      <td>-0.027418</td>\n",
              "      <td>0.075946</td>\n",
              "      <td>-0.157498</td>\n",
              "      <td>0.133206</td>\n",
              "      <td>0.089501</td>\n",
              "      <td>-0.009477</td>\n",
              "      <td>-0.029782</td>\n",
              "      <td>-0.073314</td>\n",
              "      <td>-0.125274</td>\n",
              "      <td>-0.027989</td>\n",
              "      <td>-0.126934</td>\n",
              "      <td>-0.109069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2   ...        97        98        99\n",
              "shall -0.012422 -0.020036 -0.017483  ... -0.017947 -0.004696  0.018255\n",
              "unto   0.034100  0.180420 -0.441217  ...  0.254126 -0.141179 -0.051032\n",
              "lord   0.061889  0.077935 -0.281119  ...  0.215920 -0.139021 -0.034943\n",
              "thou   0.090094  0.344583 -0.355620  ...  0.185531 -0.040391  0.029416\n",
              "thy    0.078411  0.065561 -0.414808  ... -0.027989 -0.126934 -0.109069\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrqtmERYkn-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c2495cbb-c5e5-4736-97fd-fc3bb2cf684c"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['good', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12425, 12425)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'egypt': ['aaron', 'servant', 'dwell', 'life', 'tell'],\n",
              " 'famine': ['esther', 'age', 'evermore', 'wept', 'armies'],\n",
              " 'good': ['anger', 'inhabitants', 'sight', 'servant', 'twenty'],\n",
              " 'gospel': ['gibeon', 'azariah', 'roar', 'carcases', 'joyful'],\n",
              " 'jesus': ['give', 'sight', 'christ', 'daughter', 'serve'],\n",
              " 'john': ['fled', 'except', 'false', 'gibeon', 'seventy'],\n",
              " 'moses': ['toward', 'door', 'began', 'help', 'nations'],\n",
              " 'noah': ['nathan', 'merchandise', 'insomuch', 'corruption', 'excess']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X3p12jLlFoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}