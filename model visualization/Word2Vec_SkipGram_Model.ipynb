{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec: SkipGram Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NISzsIFOIW__",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LboYzLtSh-D",
        "colab_type": "code",
        "outputId": "97e827d0-8dc0-484e-9d67-574b8f968a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpFBxs0TXuyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4C_WFRQDGS5",
        "colab_type": "code",
        "outputId": "f647cfe5-cd76-443b-bdc9-fb8764aad3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from string import punctuation\n",
        "\n",
        "bible = gutenberg.sents('bible-kjv.txt') \n",
        "remove_terms = punctuation + '0123456789'\n",
        "\n",
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
        "\n",
        "print('Total lines:', len(bible))\n",
        "print('\\nSample line:', bible[10])\n",
        "print('\\nProcessed line:', norm_bible[10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total lines: 30103\n",
            "\n",
            "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
            "\n",
            "Processed line: god said let firmament midst waters let divide waters waters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1567Lg1jOnH8",
        "colab_type": "code",
        "outputId": "43caa3b7-cb80-4f88-9bd2-0dbd2029d490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "# build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULtFu7m_TMna",
        "colab_type": "text"
      },
      "source": [
        "# Build a skip-gram [(target, context), relevancy] generator\n",
        "\n",
        "For this, we feed our skip-gram model pairs of (X, Y) where X is our input and Y is our label. We do this by using [(target, context), 1] pairs as positive input samples where target is our word of interest and context is a context word occurring near the target word and the positive label 1 indicates this is a contextually relevant pair. We also feed in [(target, random), 0] pairs as negative input samples where target is again our word of interest but random is just a randomly selected word from our vocabulary which has no context or association with our target word. Hence the negative label 0 indicates this is a contextually irrelevant pair. We do this so that the model can then learn which pairs of words are contextually relevant and which are not and generate similar embeddings for semantically similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtLA4k9eR6tM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3f41b0fb-730f-4a77-f34c-6d77f6d93ab3"
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "# generate skip-grams\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(bible (5766), james (1154)) -> 1\n",
            "(james (1154), rohgah (9806)) -> 0\n",
            "(bible (5766), korahite (9872)) -> 0\n",
            "(king (13), eber (2798)) -> 0\n",
            "(james (1154), bible (5766)) -> 1\n",
            "(king (13), bible (5766)) -> 1\n",
            "(king (13), james (1154)) -> 1\n",
            "(bible (5766), king (13)) -> 1\n",
            "(james (1154), amends (8715)) -> 0\n",
            "(james (1154), king (13)) -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaqExYSn3P1M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cec9e0a8-9e08-4a06-a800-bf39b699260a"
      },
      "source": [
        "len(skip_grams)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29251"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA17acve3R3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "699e6f75-8caf-41f9-c731-5ae026942511"
      },
      "source": [
        "skip_grams[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[5766, 1154],\n",
              "  [1154, 9806],\n",
              "  [5766, 9872],\n",
              "  [13, 2798],\n",
              "  [1154, 5766],\n",
              "  [13, 5766],\n",
              "  [13, 1154],\n",
              "  [5766, 13],\n",
              "  [1154, 8715],\n",
              "  [1154, 13],\n",
              "  [13, 4529],\n",
              "  [5766, 8840]],\n",
              " [1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urrMuWWJLkns",
        "colab_type": "text"
      },
      "source": [
        "# Building the skip-gram model architecture\n",
        "\n",
        " For this our inputs will be our target word and context or random word pair. Each of which are passed to an embedding layer (initialized with random weights) of it’s own. Once we obtain the word embeddings for the target and the context word, we pass it to a merge layer where we compute the dot product of these two vectors. Then we pass on this dot product value to a dense sigmoid layer which predicts either a 1 or a 0 depending on if the pair of words are contextually relevant or just random words (Y’). We match this with the actual relevance label (Y), compute the loss by leveraging the mean_squared_error loss and perform backpropagation with each epoch to update the embedding layer in the process. Following code shows us our model architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K4XIqA7MQ2M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "ebac8a97-7f86-42d6-dec3-db33de9ef9d9"
      },
      "source": [
        "from keras.layers import dot\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, Input\n",
        "from keras.models import Model\n",
        "\n",
        "input_1 = Input(shape=(1,))\n",
        "input_2 = Input(shape=(1,))\n",
        "\n",
        "# build skip-gram architecture\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "word_model.summary()\n",
        "\n",
        "out1 = word_model(input_1)\n",
        "\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "out2 = context_model(input_2)\n",
        "\n",
        "x = dot([out1, out2], axes=1)\n",
        "x = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(x)\n",
        "\n",
        "model =  Model(inputs=[input_1, input_2], outputs=x)\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
        "                  rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_47 (Embedding)     (None, 1, 100)            1242500   \n",
            "_________________________________________________________________\n",
            "reshape_47 (Reshape)         (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 1,242,500\n",
            "Trainable params: 1,242,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_27 (InputLayer)           (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_63 (Sequential)      (None, 100)          1242500     input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "sequential_64 (Sequential)      (None, 100)          1242500     input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_16 (Dot)                    (None, 1)            0           sequential_63[1][0]              \n",
            "                                                                 sequential_64[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            2           dot_16[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,485,002\n",
            "Trainable params: 2,485,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"304pt\" viewBox=\"0.00 0.00 470.00 304.00\" width=\"470pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 466,-300 466,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139829279639032 -->\n<g class=\"node\" id=\"node1\">\n<title>139829279639032</title>\n<polygon fill=\"none\" points=\"6,-249.5 6,-295.5 216,-295.5 216,-249.5 6,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"46\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"86,-249.5 86,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"86,-272.5 144,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"144,-249.5 144,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"144,-272.5 216,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180\" y=\"-257.3\">(None, 1)</text>\n</g>\n<!-- 139829279300072 -->\n<g class=\"node\" id=\"node3\">\n<title>139829279300072</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 222,-212.5 222,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"38.5\" y=\"-185.8\">Sequential</text>\n<polyline fill=\"none\" points=\"77,-166.5 77,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"77,-189.5 135,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"135,-166.5 135,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-197.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"135,-189.5 222,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139829279639032&#45;&gt;139829279300072 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139829279639032-&gt;139829279300072</title>\n<path d=\"M111,-249.3799C111,-241.1745 111,-231.7679 111,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"114.5001,-222.784 111,-212.784 107.5001,-222.784 114.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139829281723112 -->\n<g class=\"node\" id=\"node2\">\n<title>139829281723112</title>\n<polygon fill=\"none\" points=\"246,-249.5 246,-295.5 456,-295.5 456,-249.5 246,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"326,-249.5 326,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"326,-272.5 384,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"355\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"384,-249.5 384,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"384,-272.5 456,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-257.3\">(None, 1)</text>\n</g>\n<!-- 139829279300800 -->\n<g class=\"node\" id=\"node4\">\n<title>139829279300800</title>\n<polygon fill=\"none\" points=\"240,-166.5 240,-212.5 462,-212.5 462,-166.5 240,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-185.8\">Sequential</text>\n<polyline fill=\"none\" points=\"317,-166.5 317,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"317,-189.5 375,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"375,-166.5 375,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-197.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"375,-189.5 462,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 139829281723112&#45;&gt;139829279300800 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139829281723112-&gt;139829279300800</title>\n<path d=\"M351,-249.3799C351,-241.1745 351,-231.7679 351,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"354.5001,-222.784 351,-212.784 347.5001,-222.784 354.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279299848 -->\n<g class=\"node\" id=\"node5\">\n<title>139829279299848</title>\n<polygon fill=\"none\" points=\"96,-83.5 96,-129.5 366,-129.5 366,-83.5 96,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"115\" y=\"-102.8\">Dot</text>\n<polyline fill=\"none\" points=\"134,-83.5 134,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"134,-106.5 192,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"192,-83.5 192,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279\" y=\"-114.3\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" points=\"192,-106.5 366,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279\" y=\"-91.3\">(None, 1)</text>\n</g>\n<!-- 139829279300072&#45;&gt;139829279299848 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139829279300072-&gt;139829279299848</title>\n<path d=\"M144.4267,-166.3799C158.224,-156.8367 174.3707,-145.6686 188.9576,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"191.103,-138.3511 197.3364,-129.784 187.121,-132.594 191.103,-138.3511\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279300800&#45;&gt;139829279299848 -->\n<g class=\"edge\" id=\"edge4\">\n<title>139829279300800-&gt;139829279299848</title>\n<path d=\"M317.5733,-166.3799C303.776,-156.8367 287.6293,-145.6686 273.0424,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"274.879,-132.594 264.6636,-129.784 270.897,-138.3511 274.879,-132.594\" stroke=\"#000000\"/>\n</g>\n<!-- 139829279298952 -->\n<g class=\"node\" id=\"node6\">\n<title>139829279298952</title>\n<polygon fill=\"none\" points=\"140,-.5 140,-46.5 322,-46.5 322,-.5 140,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"192,-.5 192,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"192,-23.5 250,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"250,-.5 250,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-31.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"250,-23.5 322,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 139829279299848&#45;&gt;139829279298952 -->\n<g class=\"edge\" id=\"edge5\">\n<title>139829279299848-&gt;139829279298952</title>\n<path d=\"M231,-83.3799C231,-75.1745 231,-65.7679 231,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"234.5001,-56.784 231,-46.784 227.5001,-56.784 234.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRHusQnbfd0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3d17a99e-03ff-43ed-88e7-3735464878ec"
      },
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0710 06:13:51.172223 139832561526656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0710 06:13:51.204910 139832561526656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo77_6flhEXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}